{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "ZB8fpDkDHkav"
      },
      "outputs": [],
      "source": [
        "%matplotlib inline\n",
        "import torch\n",
        "import torchvision\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Importing Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "dataset_test = torch.utils.data.DataLoader(\n",
        "  torchvision.datasets.MNIST('../data', train=False, download=True, transform=torchvision.transforms.ToTensor()), \n",
        "  batch_size=100,\n",
        "  shuffle=True\n",
        ")\n",
        "dataset_train = torch.utils.data.DataLoader(\n",
        "  torchvision.datasets.MNIST('../data', train=True, download=True, transform=torchvision.transforms.ToTensor()),\n",
        "  batch_size=100,\n",
        "  shuffle=True\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([100, 1, 28, 28])\n"
          ]
        }
      ],
      "source": [
        "for batch_x, batch_y in dataset_train:\n",
        "    print(batch_x.shape)\n",
        "    break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JaPQjvcxEzy2"
      },
      "source": [
        "### Implementing the `accuracy` function\n",
        "\n",
        "We will evaluate the quality of a classifier by its accuracy on the test set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ILOZIfcvwspy"
      },
      "outputs": [],
      "source": [
        "def accuracy(predicted_logits, reference):\n",
        "    \"\"\"\n",
        "    Compute the ratio of correctly predicted labels\n",
        "    \n",
        "    @param predicted_logits: float32 tensor of shape (batch size, num classes)\n",
        "    @param reference: int64 tensor of shape (batch_size) with the class number\n",
        "    \"\"\"\n",
        "    \n",
        "    return (reference.numel() - torch.count_nonzero(predicted_logits.argmax(axis=1) - reference))/reference.numel()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T06qNribrErv"
      },
      "outputs": [],
      "source": [
        "def train(model, criterion, dataset_train, dataset_test, optimizer, num_epochs):\n",
        "  \"\"\"\n",
        "  @param model: torch.nn.Module\n",
        "  @param criterion: torch.nn.modules.loss._Loss\n",
        "  @param dataset_train: torch.utils.data.DataLoader\n",
        "  @param dataset_test: torch.utils.data.DataLoader\n",
        "  @param optimizer: torch.optim.Optimizer\n",
        "  @param num_epochs: int\n",
        "  \"\"\"\n",
        "  print(\"Starting training\")\n",
        "  for epoch in range(num_epochs):\n",
        "    # Train an epoch\n",
        "    model.train()\n",
        "    for batch_x, batch_y in dataset_train:\n",
        "      batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n",
        "\n",
        "      # Evaluate the network (forward pass)\n",
        "      optimizer.zero_grad()\n",
        "      output = model(batch_x)\n",
        "      \n",
        "      # Compute the gradient\n",
        "      loss = criterion(output,batch_y)\n",
        "      loss.backward() # backward pass\n",
        "\n",
        "      # Update the parameters of the model with a gradient step\n",
        "      optimizer.step() #gradient descent\n",
        "      # my_lr_scheduler.step()\n",
        "\n",
        "    # Test the quality on the test set\n",
        "    model.eval()\n",
        "    accuracies_test = []\n",
        "    for batch_x, batch_y in dataset_test:\n",
        "      batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n",
        "\n",
        "      # Evaluate the network (forward pass)\n",
        "      prediction = model(batch_x)\n",
        "      accuracies_test.append(accuracy(prediction, batch_y))\n",
        "\n",
        "    print(\"Epoch {} | Test accuracy: {:.5f}\".format(epoch, sum(accuracies_test).item()/len(accuracies_test)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "num_epochs = 10\n",
        "learning_rate = 1e-2\n",
        "batch_size = 1000\n",
        "\n",
        "# If a GPU is available (should be on Colab, we will use it)\n",
        "if not torch.cuda.is_available():\n",
        "  raise Exception(\"Things will go much quicker if you enable a GPU in Colab under 'Runtime / Change Runtime Type'\")\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Train the logistic regression model with the Adam optimizer\n",
        "criterion = torch.nn.CrossEntropyLoss() # this includes LogSoftmax which executes a logistic transformation\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ehy22aHoKDzf"
      },
      "source": [
        "### A convolutional network\n",
        "\n",
        "We use the tools you built before to train this simple Convolutional Network\n",
        "* first convolutional layer: 5x5 convolutions and 32 feature maps,\n",
        "* ReLu activation,\n",
        "* Max-pooling : 2x2 (with strading of 2x2),\n",
        "* second convolutional layer: 5x5 convolutions and 64 feature maps,\n",
        "* ReLu activation,\n",
        "* Max-pooling : 2x2 (with strading of 2x2),\n",
        "* first fully-connected layer: 512 hidden units,\n",
        "* dropout probability during training: 50 %,\n",
        "* ReLu activation,\n",
        "* TODO nb_sorties with log softmax activation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Moh7XMmtJc7n"
      },
      "outputs": [],
      "source": [
        "class CNN_Model(torch.nn.Module):\n",
        "  def __init__(self):\n",
        "    \"\"\"This architecture is a CNN\"\"\"\n",
        "    super().__init__()\n",
        "    \n",
        "    final_channels = 64\n",
        "    subsize = 16\n",
        "    self.fc_in_size = int(subsize**2 / 2**2 / 2**2 * final_channels)\n",
        "\n",
        "    self.conv1 = torch.nn.Conv2d(3, 32, 5, padding='same')\n",
        "    self.conv2 = torch.nn.Conv2d(32, final_channels, 5, padding='same')\n",
        "    \n",
        "    self.fc1 = torch.nn.Linear(self.fc_in_size, 512)\n",
        "    self.fc2 = torch.nn.Linear(512, 2)\n",
        "\n",
        "  def forward(self, x):\n",
        "    relu = torch.nn.functional.relu\n",
        "    max_pool2d = torch.nn.functional.max_pool2d(kernel_size=2, stride=2)\n",
        "\n",
        "    x = max_pool2d(relu(self.conv1(x)))\n",
        "    x = max_pool2d(relu(self.conv2(x)))\n",
        "    x = x.view(-1, self.fc_in_size)\n",
        "    x = relu(self.fc1(x))\n",
        "    x = torch.nn.functional.dropout(x, training=self.training)\n",
        "    x = self.fc2(x)\n",
        "    return torch.nn.functional.log_softmax(x, dim=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mjLWKuW_nshD"
      },
      "outputs": [],
      "source": [
        "model_lenet = CNN_Model().to(device)\n",
        "optimizer = torch.optim.Adam(model_lenet.parameters(), lr=learning_rate, weight_decay=5e-4)\n",
        "\n",
        "# decayRate = 0.95\n",
        "# my_lr_scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer=optimizer, gamma=decayRate)\n",
        "\n",
        "train(model_lenet, criterion, dataset_train, dataset_test, optimizer, num_epochs)\n",
        "\n",
        "# Expect roughly TODO X% accuracy on the test set.\n",
        "# Training should take around TODO x minutes"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "EPFL ML – Lab 10: Adversarial Robustness",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
